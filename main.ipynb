{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset/train.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "test_size = 6000\n",
    "\n",
    "data_dev = data[0:test_size].T\n",
    "Y_test = data_dev[0]\n",
    "X_test = data_dev[1:].T\n",
    "\n",
    "data_train = data[test_size:].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 784)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(X_train), len(X_train[0])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def init_params():\n",
    "    # Input to hidden layer\n",
    "    W1 = np.random.rand(10, 784) * 0.01\n",
    "    b1 = np.zeros((10, 1))\n",
    "\n",
    "    # Hidden to output layer\n",
    "    W2 = np.random.rand(10, 10) * 0.01\n",
    "    b2 = np.zeros((10, 1))\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def ReLU_derivative(Z1):\n",
    "    return (Z1 > 0).astype(float)\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    # Input to hidden layer\n",
    "    Z1 = W1.dot(X).reshape(10, 1) + b1\n",
    "    # Apply activation function (ReLU)\n",
    "    A1 = ReLU(Z1)\n",
    "\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    return Z1, A1, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.07261937],\n",
       "       [-0.3866211 ],\n",
       "       [-1.00663289],\n",
       "       [-0.35389358],\n",
       "       [-0.10850241],\n",
       "       [-0.09430202],\n",
       "       [-1.75701781],\n",
       "       [-1.88116792],\n",
       "       [-0.36423385],\n",
       "       [-1.11765724]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.random.rand(10, 1)\n",
    "\n",
    "np.log(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(A2, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = -np.sum(Y * np.log(A2 + 1e-15)) / m\n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_W2(A1, A2, Y):\n",
    "    dZ2 = A2 - Y\n",
    "    W2_grad = dZ2.dot(A1.T) / m\n",
    "\n",
    "    return W2_grad, dZ2\n",
    "\n",
    "\n",
    "def gradient_W1(dZ2, W2, X, Z1, m):\n",
    "    dZ1 = (W2.T.dot(dZ2)) * ReLU_derivative(Z1)\n",
    "    W1_grad = dZ1.dot(X.reshape(1, 784)) / m\n",
    "\n",
    "    return W1_grad, dZ1\n",
    "\n",
    "\n",
    "def b_grad(dZ, m):\n",
    "    return np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "\n",
    "def one_hot_encode(label, num_classes=10):\n",
    "    one_hot = np.zeros((num_classes, 1))\n",
    "    one_hot[label] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def back_prop(W1, b1, W2, b2, X, Y):\n",
    "    Z1, A1, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "\n",
    "    m = 1\n",
    "\n",
    "    W2_grad, dZ2 = gradient_W2(A1, A2, Y)\n",
    "    b2_grad = b_grad(dZ2, m)\n",
    "\n",
    "    W1_grad, dZ1 = gradient_W1(dZ2, W2, X, Z1, m)\n",
    "    b1_grad = b_grad(dZ1, m)\n",
    "\n",
    "    return W1_grad, b1_grad, W2_grad, b2_grad, A2\n",
    "\n",
    "\n",
    "def update_params(W1, b1, W2, b2, W1_grad, b1_grad, W2_grad, b2_grad):\n",
    "    W1 -= learning_rate * W1_grad\n",
    "    b1 -= learning_rate * b1_grad\n",
    "    W2 -= learning_rate * W2_grad\n",
    "    b2 -= learning_rate * b2_grad\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.26778852901576433\n",
      "Epoch 0, Batch 10, Loss: 0.27057670969288705\n",
      "Epoch 0, Batch 20, Loss: 0.21787045909277825\n",
      "Epoch 0, Batch 30, Loss: 0.040941055828393975\n",
      "Epoch 0, Batch 40, Loss: 0.0046705245773683915\n",
      "Epoch 0, Batch 50, Loss: 0.2663506365090585\n",
      "Epoch 0, Batch 60, Loss: 0.005440126020430303\n",
      "Epoch 0, Batch 70, Loss: 0.001192268836448814\n",
      "Epoch 0, Batch 80, Loss: 0.00663909198264076\n",
      "Epoch 0, Batch 90, Loss: 0.05928823497707819\n",
      "Epoch 0, Batch 100, Loss: 0.00323267484384699\n",
      "Epoch 0, Batch 110, Loss: 0.004616663393806768\n",
      "Epoch 0, Batch 120, Loss: 0.0034478520527875442\n",
      "Epoch 0, Batch 130, Loss: 0.0028460112281162974\n",
      "Epoch 0, Batch 140, Loss: 0.004940359078754132\n",
      "Epoch 0, Batch 150, Loss: 0.041659597458349976\n",
      "Epoch 0, Batch 160, Loss: 0.0049942558900445124\n",
      "Epoch 0, Batch 170, Loss: 0.0019049994130515863\n",
      "Epoch 0, Batch 180, Loss: 0.00036924917636252185\n",
      "Epoch 0, Batch 190, Loss: 0.000877037826046948\n",
      "Epoch 0, Batch 200, Loss: 5.0117889259164394e-05\n",
      "Epoch 0, Batch 210, Loss: 0.054189783365159686\n",
      "Epoch 0, Batch 220, Loss: 0.17723563156357594\n",
      "Epoch 0, Batch 230, Loss: 0.02517212450555677\n",
      "Epoch 0, Batch 240, Loss: 0.0013319397753080255\n",
      "Epoch 0, Batch 250, Loss: 0.14291902726833283\n",
      "Epoch 0, Batch 260, Loss: 0.0006168902649256088\n",
      "Epoch 0, Batch 270, Loss: 2.03852593456746e-05\n",
      "Epoch 0, Batch 280, Loss: 0.05192835032474348\n",
      "Epoch 0, Batch 290, Loss: 0.11005460299631117\n",
      "Epoch 0, Batch 300, Loss: 0.000277733520469823\n",
      "Epoch 0, Batch 310, Loss: 0.7428438721056934\n",
      "Epoch 0, Batch 320, Loss: 0.0009582330918018562\n",
      "Epoch 0, Batch 330, Loss: 0.00194409479158788\n",
      "Epoch 0, Batch 340, Loss: 0.011976824975838938\n",
      "Epoch 0, Batch 350, Loss: 0.03109172971121944\n",
      "Epoch 1, Batch 0, Loss: 0.03531578113882928\n",
      "Epoch 1, Batch 10, Loss: 0.0022864607298199212\n",
      "Epoch 1, Batch 20, Loss: 0.006051825596798702\n",
      "Epoch 1, Batch 30, Loss: 0.00372599451748929\n",
      "Epoch 1, Batch 40, Loss: 0.0013104880959848857\n",
      "Epoch 1, Batch 50, Loss: 0.2710016224550018\n",
      "Epoch 1, Batch 60, Loss: 0.001060823547467724\n",
      "Epoch 1, Batch 70, Loss: 7.969713972898947e-05\n",
      "Epoch 1, Batch 80, Loss: 0.005687063728264404\n",
      "Epoch 1, Batch 90, Loss: 0.03560668954924789\n",
      "Epoch 1, Batch 100, Loss: 0.0031903754272070454\n",
      "Epoch 1, Batch 110, Loss: 0.0005016656021585998\n",
      "Epoch 1, Batch 120, Loss: 0.000956285107990166\n",
      "Epoch 1, Batch 130, Loss: 0.0007860169885637567\n",
      "Epoch 1, Batch 140, Loss: 0.0004354810449101206\n",
      "Epoch 1, Batch 150, Loss: 0.005919458231212605\n",
      "Epoch 1, Batch 160, Loss: 0.0008220448147350425\n",
      "Epoch 1, Batch 170, Loss: 0.0003032370872802175\n",
      "Epoch 1, Batch 180, Loss: 0.00013639934287307354\n",
      "Epoch 1, Batch 190, Loss: 7.336673351165727e-05\n",
      "Epoch 1, Batch 200, Loss: 4.0733940989509556e-05\n",
      "Epoch 1, Batch 210, Loss: 0.08031448394177762\n",
      "Epoch 1, Batch 220, Loss: 0.05451826927934787\n",
      "Epoch 1, Batch 230, Loss: 0.02904083109779788\n",
      "Epoch 1, Batch 240, Loss: 0.0003859239288186712\n",
      "Epoch 1, Batch 250, Loss: 0.05641554374228643\n",
      "Epoch 1, Batch 260, Loss: 0.002429855594372936\n",
      "Epoch 1, Batch 270, Loss: 9.902179549535416e-06\n",
      "Epoch 1, Batch 280, Loss: 0.01635385961244502\n",
      "Epoch 1, Batch 290, Loss: 0.08403755379108749\n",
      "Epoch 1, Batch 300, Loss: 0.0001355233757480065\n",
      "Epoch 1, Batch 310, Loss: 0.15027158156236223\n",
      "Epoch 1, Batch 320, Loss: 0.00021080639674056603\n",
      "Epoch 1, Batch 330, Loss: 0.0016748491185967975\n",
      "Epoch 1, Batch 340, Loss: 0.0009635465049480412\n",
      "Epoch 1, Batch 350, Loss: 0.049853926828781606\n",
      "Epoch 2, Batch 0, Loss: 0.0031073876597156763\n",
      "Epoch 2, Batch 10, Loss: 0.0019044115928892856\n",
      "Epoch 2, Batch 20, Loss: 0.003160685851774646\n",
      "Epoch 2, Batch 30, Loss: 0.000523531980417974\n",
      "Epoch 2, Batch 40, Loss: 0.00016124626531677594\n",
      "Epoch 2, Batch 50, Loss: 0.4121000052851748\n",
      "Epoch 2, Batch 60, Loss: 0.0003775809404980339\n",
      "Epoch 2, Batch 70, Loss: 0.0001386941705157697\n",
      "Epoch 2, Batch 80, Loss: 0.002201293492712444\n",
      "Epoch 2, Batch 90, Loss: 0.02705314009433201\n",
      "Epoch 2, Batch 100, Loss: 0.0012779285558543263\n",
      "Epoch 2, Batch 110, Loss: 0.00025867748422882296\n",
      "Epoch 2, Batch 120, Loss: 0.0005354060164143199\n",
      "Epoch 2, Batch 130, Loss: 0.00048711832471486533\n",
      "Epoch 2, Batch 140, Loss: 0.0006016846563422008\n",
      "Epoch 2, Batch 150, Loss: 0.006546550782118165\n",
      "Epoch 2, Batch 160, Loss: 0.0002932710589843335\n",
      "Epoch 2, Batch 170, Loss: 7.7941005413887e-05\n",
      "Epoch 2, Batch 180, Loss: 0.00015101801654388017\n",
      "Epoch 2, Batch 190, Loss: 7.933132454515086e-05\n",
      "Epoch 2, Batch 200, Loss: 7.391172824905388e-05\n",
      "Epoch 2, Batch 210, Loss: 0.0901856245140672\n",
      "Epoch 2, Batch 220, Loss: 0.03125790979768152\n",
      "Epoch 2, Batch 230, Loss: 0.11342782743660529\n",
      "Epoch 2, Batch 240, Loss: 0.0002945826757594209\n",
      "Epoch 2, Batch 250, Loss: 0.018023325546674254\n",
      "Epoch 2, Batch 260, Loss: 0.0006761755741039244\n",
      "Epoch 2, Batch 270, Loss: 9.399083900427703e-06\n",
      "Epoch 2, Batch 280, Loss: 0.015611684426843533\n",
      "Epoch 2, Batch 290, Loss: 0.10697304946939734\n",
      "Epoch 2, Batch 300, Loss: 0.0007798938532632973\n",
      "Epoch 2, Batch 310, Loss: 0.12452906300161141\n",
      "Epoch 2, Batch 320, Loss: 8.756715152202534e-05\n",
      "Epoch 2, Batch 330, Loss: 0.0033969103843450787\n",
      "Epoch 2, Batch 340, Loss: 0.00018442239836965365\n",
      "Epoch 2, Batch 350, Loss: 0.05648875066531974\n",
      "Epoch 3, Batch 0, Loss: 0.0005855612025317972\n",
      "Epoch 3, Batch 10, Loss: 0.0007146961562603305\n",
      "Epoch 3, Batch 20, Loss: 0.0012388906591983878\n",
      "Epoch 3, Batch 30, Loss: 0.00041468053336824243\n",
      "Epoch 3, Batch 40, Loss: 6.916531304057621e-05\n",
      "Epoch 3, Batch 50, Loss: 0.37521209256574695\n",
      "Epoch 3, Batch 60, Loss: 0.00034060572864184766\n",
      "Epoch 3, Batch 70, Loss: 5.176610419165257e-05\n",
      "Epoch 3, Batch 80, Loss: 0.0010150723721007418\n",
      "Epoch 3, Batch 90, Loss: 0.033478255301784124\n",
      "Epoch 3, Batch 100, Loss: 0.0007564156892798455\n",
      "Epoch 3, Batch 110, Loss: 0.00048011360910004536\n",
      "Epoch 3, Batch 120, Loss: 0.0001676654888320712\n",
      "Epoch 3, Batch 130, Loss: 0.00035111523621725744\n",
      "Epoch 3, Batch 140, Loss: 0.0005976056258669407\n",
      "Epoch 3, Batch 150, Loss: 0.01072500093472656\n",
      "Epoch 3, Batch 160, Loss: 0.00013567832224583746\n",
      "Epoch 3, Batch 170, Loss: 1.8498504885970947e-05\n",
      "Epoch 3, Batch 180, Loss: 0.0002651635731041748\n",
      "Epoch 3, Batch 190, Loss: 2.0127434573857015e-05\n",
      "Epoch 3, Batch 200, Loss: 0.00011990230577058781\n",
      "Epoch 3, Batch 210, Loss: 0.10414804194812426\n",
      "Epoch 3, Batch 220, Loss: 0.01679812557787843\n",
      "Epoch 3, Batch 230, Loss: 0.19868031406044398\n",
      "Epoch 3, Batch 240, Loss: 0.00024801248076097355\n",
      "Epoch 3, Batch 250, Loss: 0.02997781112615223\n",
      "Epoch 3, Batch 260, Loss: 0.00033169104384670763\n",
      "Epoch 3, Batch 270, Loss: 8.461455758431206e-06\n",
      "Epoch 3, Batch 280, Loss: 0.009784178351408901\n",
      "Epoch 3, Batch 290, Loss: 0.09996879550465357\n",
      "Epoch 3, Batch 300, Loss: 0.0005549478058925413\n",
      "Epoch 3, Batch 310, Loss: 0.02257414087993818\n",
      "Epoch 3, Batch 320, Loss: 0.00019190210406622182\n",
      "Epoch 3, Batch 330, Loss: 0.0015183386160895298\n",
      "Epoch 3, Batch 340, Loss: 0.0001494679349326671\n",
      "Epoch 3, Batch 350, Loss: 0.06701615005617326\n",
      "Epoch 4, Batch 0, Loss: 5.827403161576042e-05\n",
      "Epoch 4, Batch 10, Loss: 0.00026319908874383773\n",
      "Epoch 4, Batch 20, Loss: 0.0013402467573308515\n",
      "Epoch 4, Batch 30, Loss: 0.0007377023206906448\n",
      "Epoch 4, Batch 40, Loss: 4.407535346918867e-05\n",
      "Epoch 4, Batch 50, Loss: 0.4366902583346473\n",
      "Epoch 4, Batch 60, Loss: 0.0005599485737721127\n",
      "Epoch 4, Batch 70, Loss: 2.9508578129267813e-05\n",
      "Epoch 4, Batch 80, Loss: 0.000750756123038702\n",
      "Epoch 4, Batch 90, Loss: 0.022114405859503684\n",
      "Epoch 4, Batch 100, Loss: 0.00028643711628732743\n",
      "Epoch 4, Batch 110, Loss: 0.0009491438658526943\n",
      "Epoch 4, Batch 120, Loss: 6.525776399564377e-05\n",
      "Epoch 4, Batch 130, Loss: 0.00028379079058530443\n",
      "Epoch 4, Batch 140, Loss: 0.0008844641048669094\n",
      "Epoch 4, Batch 150, Loss: 0.01261633738131435\n",
      "Epoch 4, Batch 160, Loss: 7.998595629468905e-05\n",
      "Epoch 4, Batch 170, Loss: 6.595950912631374e-05\n",
      "Epoch 4, Batch 180, Loss: 0.0005902104970062262\n",
      "Epoch 4, Batch 190, Loss: 1.519533524107796e-05\n",
      "Epoch 4, Batch 200, Loss: 6.809321638740875e-05\n",
      "Epoch 4, Batch 210, Loss: 0.09286985325173802\n",
      "Epoch 4, Batch 220, Loss: 0.01805917628215267\n",
      "Epoch 4, Batch 230, Loss: 0.276226602430265\n",
      "Epoch 4, Batch 240, Loss: 0.0003998359380848104\n",
      "Epoch 4, Batch 250, Loss: 0.044960824393193204\n",
      "Epoch 4, Batch 260, Loss: 0.00029399631358860195\n",
      "Epoch 4, Batch 270, Loss: 1.0345558723558869e-05\n",
      "Epoch 4, Batch 280, Loss: 0.008922504546930093\n",
      "Epoch 4, Batch 290, Loss: 0.10978356254619828\n",
      "Epoch 4, Batch 300, Loss: 0.0003341198610407272\n",
      "Epoch 4, Batch 310, Loss: 0.0018182771489710058\n",
      "Epoch 4, Batch 320, Loss: 0.00030300666919633147\n",
      "Epoch 4, Batch 330, Loss: 0.0009095284726694228\n",
      "Epoch 4, Batch 340, Loss: 0.00013482358345314849\n",
      "Epoch 4, Batch 350, Loss: 0.09353070252492238\n",
      "Epoch 5, Batch 0, Loss: 1.418160993088085e-05\n",
      "Epoch 5, Batch 10, Loss: 0.00010978650976759899\n",
      "Epoch 5, Batch 20, Loss: 0.0009784411252627056\n",
      "Epoch 5, Batch 30, Loss: 0.0011581321703190042\n",
      "Epoch 5, Batch 40, Loss: 1.180510039428869e-05\n",
      "Epoch 5, Batch 50, Loss: 0.4448903672904841\n",
      "Epoch 5, Batch 60, Loss: 0.0003005047062647929\n",
      "Epoch 5, Batch 70, Loss: 2.6942659950274195e-05\n",
      "Epoch 5, Batch 80, Loss: 0.000568833117165382\n",
      "Epoch 5, Batch 90, Loss: 0.013734435240777564\n",
      "Epoch 5, Batch 100, Loss: 0.0002029308149611231\n",
      "Epoch 5, Batch 110, Loss: 0.0009417230835797668\n",
      "Epoch 5, Batch 120, Loss: 2.6886861910885728e-05\n",
      "Epoch 5, Batch 130, Loss: 0.00024662971849342806\n",
      "Epoch 5, Batch 140, Loss: 0.001374371087081995\n",
      "Epoch 5, Batch 150, Loss: 0.015108067589808396\n",
      "Epoch 5, Batch 160, Loss: 0.00012515012694952883\n",
      "Epoch 5, Batch 170, Loss: 8.079689885610419e-05\n",
      "Epoch 5, Batch 180, Loss: 0.0007078821464668124\n",
      "Epoch 5, Batch 190, Loss: 3.3199536598432375e-06\n",
      "Epoch 5, Batch 200, Loss: 9.527871053752508e-05\n",
      "Epoch 5, Batch 210, Loss: 0.0672595228524044\n",
      "Epoch 5, Batch 220, Loss: 0.02120398851796717\n",
      "Epoch 5, Batch 230, Loss: 0.20753128276429295\n",
      "Epoch 5, Batch 240, Loss: 0.00030194511864081426\n",
      "Epoch 5, Batch 250, Loss: 0.022059454478109638\n",
      "Epoch 5, Batch 260, Loss: 0.00024868537548566845\n",
      "Epoch 5, Batch 270, Loss: 7.512382233825663e-06\n",
      "Epoch 5, Batch 280, Loss: 0.007591795793365115\n",
      "Epoch 5, Batch 290, Loss: 0.11431242208584869\n",
      "Epoch 5, Batch 300, Loss: 0.00030185097513226296\n",
      "Epoch 5, Batch 310, Loss: 0.001317278779876504\n",
      "Epoch 5, Batch 320, Loss: 0.0005641754521484619\n",
      "Epoch 5, Batch 330, Loss: 0.0008465167772550833\n",
      "Epoch 5, Batch 340, Loss: 0.00015157853277928117\n",
      "Epoch 5, Batch 350, Loss: 0.11365389184951853\n",
      "Epoch 6, Batch 0, Loss: 1.1602674056238216e-05\n",
      "Epoch 6, Batch 10, Loss: 8.64624433092815e-05\n",
      "Epoch 6, Batch 20, Loss: 0.0013435496926047254\n",
      "Epoch 6, Batch 30, Loss: 0.0011116833098419735\n",
      "Epoch 6, Batch 40, Loss: 7.216842849221896e-06\n",
      "Epoch 6, Batch 50, Loss: 0.48640438162212307\n",
      "Epoch 6, Batch 60, Loss: 0.0003427388889204181\n",
      "Epoch 6, Batch 70, Loss: 2.4236726982742014e-05\n",
      "Epoch 6, Batch 80, Loss: 0.0004315115575197862\n",
      "Epoch 6, Batch 90, Loss: 0.011444537055644206\n",
      "Epoch 6, Batch 100, Loss: 0.0002558465526713489\n",
      "Epoch 6, Batch 110, Loss: 0.0010974440119166592\n",
      "Epoch 6, Batch 120, Loss: 2.5471364341438276e-05\n",
      "Epoch 6, Batch 130, Loss: 0.00017562667878091002\n",
      "Epoch 6, Batch 140, Loss: 0.0012816129701053492\n",
      "Epoch 6, Batch 150, Loss: 0.016329228477859635\n",
      "Epoch 6, Batch 160, Loss: 0.00012712110210421295\n",
      "Epoch 6, Batch 170, Loss: 4.941400149324671e-05\n",
      "Epoch 6, Batch 180, Loss: 0.0006884037169263924\n",
      "Epoch 6, Batch 190, Loss: 4.258878448832841e-06\n",
      "Epoch 6, Batch 200, Loss: 0.00010058323410092022\n",
      "Epoch 6, Batch 210, Loss: 0.05318964919142076\n",
      "Epoch 6, Batch 220, Loss: 0.01903329771374826\n",
      "Epoch 6, Batch 230, Loss: 0.19478008422333604\n",
      "Epoch 6, Batch 240, Loss: 0.0003524457197646716\n",
      "Epoch 6, Batch 250, Loss: 0.011850580457811828\n",
      "Epoch 6, Batch 260, Loss: 0.00028053071226729167\n",
      "Epoch 6, Batch 270, Loss: 8.654117894273766e-06\n",
      "Epoch 6, Batch 280, Loss: 0.004526047892357109\n",
      "Epoch 6, Batch 290, Loss: 0.15896992541682245\n",
      "Epoch 6, Batch 300, Loss: 0.0002136380239381484\n",
      "Epoch 6, Batch 310, Loss: 0.0001009071074076209\n",
      "Epoch 6, Batch 320, Loss: 0.0006611442945275703\n",
      "Epoch 6, Batch 330, Loss: 0.0005983572070818305\n",
      "Epoch 6, Batch 340, Loss: 8.659891882318611e-05\n",
      "Epoch 6, Batch 350, Loss: 0.13158586053500543\n",
      "Epoch 7, Batch 0, Loss: 9.760327670560476e-06\n",
      "Epoch 7, Batch 10, Loss: 6.793232039667135e-05\n",
      "Epoch 7, Batch 20, Loss: 0.0012390707878397887\n",
      "Epoch 7, Batch 30, Loss: 0.0012027872448765612\n",
      "Epoch 7, Batch 40, Loss: 4.153040628672987e-06\n",
      "Epoch 7, Batch 50, Loss: 0.5436549994021249\n",
      "Epoch 7, Batch 60, Loss: 0.0003702098898770344\n",
      "Epoch 7, Batch 70, Loss: 2.2338806481818676e-05\n",
      "Epoch 7, Batch 80, Loss: 0.0002903971694369243\n",
      "Epoch 7, Batch 90, Loss: 0.007356147321800947\n",
      "Epoch 7, Batch 100, Loss: 0.00015769728793614904\n",
      "Epoch 7, Batch 110, Loss: 0.001363982043699829\n",
      "Epoch 7, Batch 120, Loss: 3.5278928471401884e-05\n",
      "Epoch 7, Batch 130, Loss: 0.00018108146461811773\n",
      "Epoch 7, Batch 140, Loss: 0.0011996539766805682\n",
      "Epoch 7, Batch 150, Loss: 0.014813499599883345\n",
      "Epoch 7, Batch 160, Loss: 0.00011232827952617865\n",
      "Epoch 7, Batch 170, Loss: 7.826900411224642e-05\n",
      "Epoch 7, Batch 180, Loss: 0.0003956434502213685\n",
      "Epoch 7, Batch 190, Loss: 2.338731661425124e-06\n",
      "Epoch 7, Batch 200, Loss: 7.804218920901176e-05\n",
      "Epoch 7, Batch 210, Loss: 0.04755859971498567\n",
      "Epoch 7, Batch 220, Loss: 0.01694184856939982\n",
      "Epoch 7, Batch 230, Loss: 0.19615504145382023\n",
      "Epoch 7, Batch 240, Loss: 0.0003666143095005939\n",
      "Epoch 7, Batch 250, Loss: 0.010975062912914044\n",
      "Epoch 7, Batch 260, Loss: 0.00024229593137698127\n",
      "Epoch 7, Batch 270, Loss: 9.506273147753151e-06\n",
      "Epoch 7, Batch 280, Loss: 0.0032102332457924674\n",
      "Epoch 7, Batch 290, Loss: 0.14554370998466984\n",
      "Epoch 7, Batch 300, Loss: 0.0002872920550802707\n",
      "Epoch 7, Batch 310, Loss: 2.22035219363769e-05\n",
      "Epoch 7, Batch 320, Loss: 0.0009024775198628197\n",
      "Epoch 7, Batch 330, Loss: 0.0003953416698435396\n",
      "Epoch 7, Batch 340, Loss: 7.014787758261413e-05\n",
      "Epoch 7, Batch 350, Loss: 0.1658268372810127\n",
      "Epoch 8, Batch 0, Loss: 6.924250118001087e-06\n",
      "Epoch 8, Batch 10, Loss: 5.564201391959426e-05\n",
      "Epoch 8, Batch 20, Loss: 0.0013072369353106794\n",
      "Epoch 8, Batch 30, Loss: 0.001153227640895822\n",
      "Epoch 8, Batch 40, Loss: 3.113823301468666e-06\n",
      "Epoch 8, Batch 50, Loss: 0.5682167238654884\n",
      "Epoch 8, Batch 60, Loss: 0.00014404881623440515\n",
      "Epoch 8, Batch 70, Loss: 2.7117227506437424e-05\n",
      "Epoch 8, Batch 80, Loss: 0.00031810290527198597\n",
      "Epoch 8, Batch 90, Loss: 0.005073179839035563\n",
      "Epoch 8, Batch 100, Loss: 8.536551733890054e-05\n",
      "Epoch 8, Batch 110, Loss: 0.001347277030208977\n",
      "Epoch 8, Batch 120, Loss: 2.5662911104911893e-05\n",
      "Epoch 8, Batch 130, Loss: 0.00014627059026321444\n",
      "Epoch 8, Batch 140, Loss: 0.000899764301092769\n",
      "Epoch 8, Batch 150, Loss: 0.01658637966376792\n",
      "Epoch 8, Batch 160, Loss: 0.00019080448957256067\n",
      "Epoch 8, Batch 170, Loss: 0.00014687630250892413\n",
      "Epoch 8, Batch 180, Loss: 0.00040505581132074646\n",
      "Epoch 8, Batch 190, Loss: 2.1014531707457815e-06\n",
      "Epoch 8, Batch 200, Loss: 7.702119105822556e-05\n",
      "Epoch 8, Batch 210, Loss: 0.05110794398833877\n",
      "Epoch 8, Batch 220, Loss: 0.017760251095352335\n",
      "Epoch 8, Batch 230, Loss: 0.17049288380283892\n",
      "Epoch 8, Batch 240, Loss: 0.0003342313093393641\n",
      "Epoch 8, Batch 250, Loss: 0.011193981764887147\n",
      "Epoch 8, Batch 260, Loss: 0.0005130475951188873\n",
      "Epoch 8, Batch 270, Loss: 5.8933963708745585e-06\n",
      "Epoch 8, Batch 280, Loss: 0.0014283958073841674\n",
      "Epoch 8, Batch 290, Loss: 0.15895212009065926\n",
      "Epoch 8, Batch 300, Loss: 0.00018675415581254775\n",
      "Epoch 8, Batch 310, Loss: 4.642399171885861e-06\n",
      "Epoch 8, Batch 320, Loss: 0.0006349430012492186\n",
      "Epoch 8, Batch 330, Loss: 0.0002905482929623621\n",
      "Epoch 8, Batch 340, Loss: 5.1582600149140794e-05\n",
      "Epoch 8, Batch 350, Loss: 0.16369058299094752\n",
      "Epoch 9, Batch 0, Loss: 8.1015316831343e-06\n",
      "Epoch 9, Batch 10, Loss: 6.744972190828143e-05\n",
      "Epoch 9, Batch 20, Loss: 0.0014367263029572582\n",
      "Epoch 9, Batch 30, Loss: 0.001231094750927716\n",
      "Epoch 9, Batch 40, Loss: 2.7512232135874024e-06\n",
      "Epoch 9, Batch 50, Loss: 0.5275517105961993\n",
      "Epoch 9, Batch 60, Loss: 0.00021285390802225262\n",
      "Epoch 9, Batch 70, Loss: 2.797008629783441e-05\n",
      "Epoch 9, Batch 80, Loss: 0.00020726948197359886\n",
      "Epoch 9, Batch 90, Loss: 0.003932474421336007\n",
      "Epoch 9, Batch 100, Loss: 5.625686598369159e-05\n",
      "Epoch 9, Batch 110, Loss: 0.0015963995964569902\n",
      "Epoch 9, Batch 120, Loss: 1.7454110265874345e-05\n",
      "Epoch 9, Batch 130, Loss: 0.00013813971022332976\n",
      "Epoch 9, Batch 140, Loss: 0.0010174434979782613\n",
      "Epoch 9, Batch 150, Loss: 0.018300128873397617\n",
      "Epoch 9, Batch 160, Loss: 0.0002181536909911218\n",
      "Epoch 9, Batch 170, Loss: 0.00015254278608812987\n",
      "Epoch 9, Batch 180, Loss: 0.00019527114868829083\n",
      "Epoch 9, Batch 190, Loss: 3.3992308914445406e-06\n",
      "Epoch 9, Batch 200, Loss: 6.736513823136676e-05\n",
      "Epoch 9, Batch 210, Loss: 0.05075190786369144\n",
      "Epoch 9, Batch 220, Loss: 0.01433835107348327\n",
      "Epoch 9, Batch 230, Loss: 0.20589973858047483\n",
      "Epoch 9, Batch 240, Loss: 0.0002661988263902816\n",
      "Epoch 9, Batch 250, Loss: 0.012595739808007803\n",
      "Epoch 9, Batch 260, Loss: 0.0009258405800289242\n",
      "Epoch 9, Batch 270, Loss: 6.026253249685563e-06\n",
      "Epoch 9, Batch 280, Loss: 0.0016516405111879704\n",
      "Epoch 9, Batch 290, Loss: 0.15546872009525742\n",
      "Epoch 9, Batch 300, Loss: 0.00019828388299513678\n",
      "Epoch 9, Batch 310, Loss: 5.744812386458929e-06\n",
      "Epoch 9, Batch 320, Loss: 0.0006920369772095247\n",
      "Epoch 9, Batch 330, Loss: 0.00029959618435858613\n",
      "Epoch 9, Batch 340, Loss: 3.824087291023227e-05\n",
      "Epoch 9, Batch 350, Loss: 0.1405583366299243\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = init_params()\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "num_examples = X_train.shape[0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # indices = np.random.perm\n",
    "\n",
    "    # for i in range(0, nutation(num_examples)\n",
    "    # X_train = X_train[indices]\n",
    "    # Y_train = Y_train[indices]um_examples, batch_size):\n",
    "    #     batch_X = X_train[i : i + batch_size].T\n",
    "    #     batch_Y = Y_train[i : i + batch_size].T\n",
    "    for i in range(0, num_examples, 1):\n",
    "        batch_X = X_train[i].T\n",
    "        batch_Y = one_hot_encode(Y_train[i])\n",
    "\n",
    "        # if batch_X.shape[1] == 0:\n",
    "        #     continue\n",
    "\n",
    "        W1_grad, b1_grad, W2_grad, b2_grad, A2 = back_prop(\n",
    "            W1, b1, W2, b2, batch_X, batch_Y\n",
    "        )\n",
    "\n",
    "        W1, b1, W2, b2 = update_params(\n",
    "            W1, b1, W2, b2, W1_grad, b1_grad, W2_grad, b2_grad\n",
    "        )\n",
    "\n",
    "        avg_loss = cross_entropy_loss(A2, batch_Y)\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i//batch_size}, Loss: {avg_loss}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.93%\n"
     ]
    }
   ],
   "source": [
    "def test_model(W1, b1, W2, b2, X_test, Y_test):\n",
    "    num_test_examples = X_test.shape[0]\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(0, num_test_examples):\n",
    "        _, _, A2 = forward_prop(W1, b1, W2, b2, X_test[i])\n",
    "\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "\n",
    "        true_labels = Y_test[i]\n",
    "\n",
    "        if predictions[0] == true_labels:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / num_test_examples * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "accuracy = test_model(W1, b1, W2, b2, X_test, Y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.56609130e-08]\n",
      " [1.71308731e-11]\n",
      " [5.45939317e-08]\n",
      " [7.64174480e-05]\n",
      " [5.39772118e-12]\n",
      " [9.99921383e-01]\n",
      " [9.71568401e-07]\n",
      " [4.15109106e-11]\n",
      " [4.03632156e-08]\n",
      " [1.06705555e-06]]\n",
      "Predicted: 5, True: 5, Correct: True\n"
     ]
    }
   ],
   "source": [
    "def test_single_example(W1, b1, W2, b2, X, Y):\n",
    "    _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    print(A2)\n",
    "    prediction = np.argmax(A2)\n",
    "    true_label = Y\n",
    "    print(\n",
    "        f\"Predicted: {prediction}, True: {true_label}, Correct: {prediction == true_label}\"\n",
    "    )\n",
    "\n",
    "\n",
    "test_single_example(W1, b1, W2, b2, X_test[0], Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 784)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = np.array(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  28000\n",
      "ids:  28000\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(W1, b1, W2, b2, X_test):\n",
    "    Y_test = []\n",
    "    num_test_examples = X_test.shape[0]\n",
    "\n",
    "    for i in range(0, num_test_examples):\n",
    "        _, _, A2 = forward_prop(W1, b1, W2, b2, X_test[i])\n",
    "\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "\n",
    "        Y_test.append(predictions[0])\n",
    "\n",
    "    return Y_test\n",
    "\n",
    "\n",
    "predictions = get_predictions(W1, b1, W2, b2, data_test)\n",
    "ids = [i for i in range(1, data_test.shape[0] + 1)]\n",
    "\n",
    "print(\"predictions: \", len(predictions))\n",
    "print(\"ids: \", len(ids))\n",
    "\n",
    "df = pd.DataFrame({\"ImageId\": ids, \"Label\": predictions})\n",
    "\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
